# -*- coding: utf-8 -*-
"""RNNArchitecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13b3S1u5GdO2W5XBDYFvzkr9kLIiej3m4
"""

from tensorflow.keras.layers import Input, SimpleRNN, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD, Adam

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Common RNN Notations
# N = number of samples
# T = sequence length
# D = number of input features
# M = number of hidden units
# K = number of output units

# Create some random data
N = 1
T = 10
D = 3
K = 2
X = np.random.randn(N, T, D)

# Defining our model
M = 5 # number of hidden units
i = Input(shape=(T, D))
x = SimpleRNN(M)(i)
x = Dense(K)(x)

model = Model(i, x)

# Get the output
Yhat = model.predict(X)
print(Yhat)

#Print the model summary
model.summary()

model.layers[1].get_weights()

# First output is input > hidden
# Second output is hidden > hidden
# Third output is bias term (vector of length M)
a, b, c = model.layers[1].get_weights()
print(a.shape, b.shape, c.shape)

a,b=model.layers[2].get_weights()
print(a,b)

# We will do manual calculation of RNN
# Assigning Weight variables

Wx, Wh, bh = model.layers[1].get_weights()
Wo, bo = model.layers[2].get_weights()

h_last = np.zeros(M) # initial hidden state
x = X[0] # the one and only sample
Yhats = [] # where we store the outputs

for t in range(T):
  h = np.tanh(x[t].dot(Wx) + h_last.dot(Wh) + bh)
  y = h.dot(Wo) + bo # we only care about this value on the last iteration
  Yhats.append(y)
  
  # important: assign h to h_last
  h_last = h

# print the final output
print(Yhats[-1])